{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1726f6c5-0b63-4184-ab57-21fad4577fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza, re, benepar, psutil, gc, json, csv, time, string, os\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from textstat import textstat\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import Tree\n",
    "from textblob import Word\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator, LogFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ee7f15-f969-4fe0-83d2-6584568b66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memcheck():\n",
    "    gc.collect()\n",
    "    memory_info = psutil.virtual_memory()\n",
    "\n",
    "    # Display the memory information in GB\n",
    "    total_memory = memory_info.total / (1024 ** 3)\n",
    "    available_memory = memory_info.available / (1024 ** 3)\n",
    "    used_memory = memory_info.used / (1024 ** 3)\n",
    "\n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "    print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "\n",
    "def time_taken(start_time, end_time):\n",
    "    time_taken = end_time - start_time\n",
    "    hours, remainder = divmod(time_taken.seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    print(f\"Time Taken: {hours} hours, {minutes} minutes, {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3533b1ab-e0f3-4845-8d3d-908237a704d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 17:13:07 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c80c4582c94af6993c8d6cfaddf5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 17:13:07 INFO: Downloaded file to C:\\Users\\Roland\\stanza_resources\\resources.json\n",
      "2024-10-28 17:13:09 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| sentiment | sstplus_charlm            |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-10-28 17:13:09 INFO: Using device: cpu\n",
      "2024-10-28 17:13:09 INFO: Loading: tokenize\n",
      "2024-10-28 17:13:11 INFO: Loading: mwt\n",
      "2024-10-28 17:13:11 INFO: Loading: pos\n",
      "2024-10-28 17:13:11 INFO: Loading: lemma\n",
      "2024-10-28 17:13:11 INFO: Loading: depparse\n",
      "2024-10-28 17:13:12 INFO: Loading: sentiment\n",
      "2024-10-28 17:13:12 INFO: Loading: ner\n",
      "2024-10-28 17:13:12 INFO: Done loading processors!\n",
      "2024-10-28 17:13:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee24ced3085646e99f7f711435cffe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 17:13:13 INFO: Downloaded file to C:\\Users\\Roland\\stanza_resources\\resources.json\n",
      "2024-10-28 17:13:13 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-10-28 17:13:13 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2024-10-28 17:13:13 INFO: Using device: cpu\n",
      "2024-10-28 17:13:13 INFO: Loading: tokenize\n",
      "2024-10-28 17:13:13 INFO: Loading: mwt\n",
      "2024-10-28 17:13:13 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse,sentiment,ner')\n",
    "nlp_tokens = stanza.Pipeline(lang='en', processors='tokenize', use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3679f96e-a99d-4162-b101-a74ea76f2a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_text_in_context(text, words, window = 75):\n",
    "#Finds the first occurrence of each word in target text and displays it\n",
    "    \n",
    "    for word in words:\n",
    "        ind = text.find(word)\n",
    "        if ind == -1:\n",
    "            print (f\"{word} not found in text\")\n",
    "        else:\n",
    "            display_text = text[ind-window:ind+window+1]\n",
    "            display_text = display_text.replace(\"\\n\", \" \")\n",
    "            display_text = display_text.replace(\"\\t\", \" \")\n",
    "            print(display_text)\n",
    "\n",
    "def display_words_in_context(text_words, words, repeat = False, window = 2):\n",
    "# Finds the first occurrence of each word in target list of words. Optionally finds every subsequent occurrence     \n",
    "    if not repeat:\n",
    "        for word in words:\n",
    "            try:\n",
    "                index = text_words.index(word)\n",
    "                #print(f\"The index of '{word}' is: {ind}\")\n",
    "                display_text = ' '.join(text_words[index - window:index])\n",
    "                display_text = display_text + \" [\" + word + \"] \"\n",
    "                display_text = display_text + ' '.join(text_words[index + 1 : index + window + 1])\n",
    "                print(display_text)\n",
    "            except ValueError:\n",
    "                print(f\"'{word}' not found in the text.\")\n",
    "    else:\n",
    "        for word in words:\n",
    "            indices = [index for index, value in enumerate(text_words) if value == word]\n",
    "            if len(indices) == 0:\n",
    "                print(f\"'{word}' not found in the text.\")\n",
    "            else:\n",
    "                for index in indices:\n",
    "                    display_text = ' '.join(text_words[index - window:index])\n",
    "                    display_text = display_text + \" [\" + word + \"] \"\n",
    "                    display_text = display_text + ' '.join(text_words[index + 1 : index + window + 1])\n",
    "                    print(display_text)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169f2aaf-8823-4555-86c9-7ea1bcdcc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def where_is(word):\n",
    "    # finds if spell checker has flagged a word.  Must run spell checker first.\n",
    "    if (word in spell_set_1): \n",
    "        print (\"In spell set 1\")\n",
    "    if (word in blob_set_1):\n",
    "        print (\"In blob set 1\")\n",
    "    if (word in spell_set_1) or (word in blob_set_1):\n",
    "        display_words_in_context(test_words_1, [word], False, 6)\n",
    "    if (word in spell_set_2): \n",
    "        print (\"In spell set 2\")\n",
    "    if (word in blob_set_2):\n",
    "        print (\"In blob  set 2\")\n",
    "    if (word in spell_set_2) or (word in blob_set_2):\n",
    "        display_words_in_context(test_words_2, [word], False, 6)\n",
    "    if (word in spell_set_3): \n",
    "        print (\"In spell set 3\")\n",
    "    if (word in blob_set_3):\n",
    "        print (\"In blob set 3\")\n",
    "    if (word in spell_set_1) or (word in blob_set_3):\n",
    "        display_words_in_context(test_words_3, [word], False, 6)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29278a47-903b-4cda-9983-e1625d9ce6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hyphenated_words(text):\n",
    "    # Use regex to find words that are hyphenated across lines\n",
    "    # Match sequences where a hyphen is at the end of a line, followed by a newline, and then continued with a word\n",
    "    hyphenated_words = re.findall(r\"(\\w+)-\\n(\\w+)\", text)\n",
    "\n",
    "    for first_part, second_part in hyphenated_words:\n",
    "        print(f\"Found broken word: {first_part}-{second_part}\")\n",
    "    print(f\"{len(hyphenated_words)} hyphenated words found, text length is: {len(text)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db25ec42-fd25-4ccb-8473-aee451fffdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_hyphenated_words(text, show_changes = False):\n",
    "    # This regular expression captures words separated by a hyphen, with letters on both sides.\n",
    "    pattern = r'(\\b\\w+)-(\\w+\\b)'\n",
    "    \n",
    "    # Function to handle replacement and printing\n",
    "    def replacement(match):\n",
    "        # Original hyphenated word\n",
    "        original_word = match.group(0)\n",
    "        # Replacement word (with space instead of hyphen)\n",
    "        altered_word = match.group(1) + \" \" + match.group(2)\n",
    "        \n",
    "        # Print the hyphenated word that was altered\n",
    "        if show_changes: print(f\"Altered: {original_word} -> {altered_word}\")\n",
    "        \n",
    "        return altered_word\n",
    "\n",
    "    # Replace hyphenated words and call the replacement function\n",
    "    result = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24cd98d-fecd-47ae-934f-11ab9a1d0ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Roland/Documents/AI/stylometry/Kazuo Ishiguro - The Remains of the Day.txt\n"
     ]
    }
   ],
   "source": [
    "source_texts= list()\n",
    "source_texts.append(\"Kazuo Ishiguro - Never Let Me Go.txt\")\n",
    "source_texts.append(\"Kazuo Ishiguro - The Remains of the Day\")\n",
    "source_texts.append(\"Kazuo Ishiguro - A Pale View of Hills-Knopf Doubleday Publishing Group (1990)\")\n",
    "source_texts.append(\"Kazuo-Ishiguro-When-We-Were-Orphans-Alfred-A.-Knopf_Vintage-_2001_\")\n",
    "source_texts.append(\"The Buried Giant (Kazuo Ishiguro) (Z-Library)-1\")\n",
    "source_texts.append(\"Kazuo Ishiguro - The Unconsoled-Vintage (1996)\")\n",
    "directory_path = \"C:/Users/Roland/Documents/AI/stylometry/\"\n",
    "file_path = directory_path+source_texts[1]+\".txt\"\n",
    "print (file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b70941-aa90-4c56-a8e1-d9f1dde577c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file not found at  C:/Users/Roland/Documents/AI/stylometry/Kazuo Ishiguro - The Remains of the Day edited_2.txt\n",
      "file found at  C:/Users/Roland/Documents/AI/stylometry/Kazuo Ishiguro - The Remains of the Day edited_1.txt\n"
     ]
    }
   ],
   "source": [
    "text_choice = 1\n",
    "\n",
    "file_path = directory_path+source_texts[text_choice]+\" edited_2.txt\"\n",
    "if os.path.isfile(file_path):\n",
    "    print (\"file found at \", file_path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    print(\"file not found at \", file_path)\n",
    "    file_path = directory_path+source_texts[text_choice]+\" edited_1.txt\"\n",
    "    if os.path.isfile(file_path):\n",
    "        print (\"file found at \", file_path)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    else:\n",
    "        print(\"file not found at \", file_path)\n",
    "        file_path = directory_path+source_texts[text_choice]+\" edited.txt\"\n",
    "        if os.path.isfile(file_path):\n",
    "            print (\"file found at \", file_path)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            print(\"file not found at \", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3be5dd1-b1c4-484d-a3f9-75d0dcd7b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423406\n"
     ]
    }
   ],
   "source": [
    "print (len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7cf453-faef-4e39-a15a-0e90509de19c",
   "metadata": {},
   "source": [
    "Prepare words for spell checking.\n",
    "\n",
    "The first set has punctuation removed\n",
    "\n",
    "The second set has punctuation removed after replacing hyphenated words with spaces\n",
    "\n",
    "The third set is also made lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "865e6cc2-e337-46a9-a4de-df1665fc9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words_1 = text.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "unhyphenated = replace_hyphenated_words(text)\n",
    "test_words_2 = unhyphenated.translate(str.maketrans('', '', string.punctuation)).split()\n",
    "test_words_3 = unhyphenated.translate(str.maketrans('', '', string.punctuation)).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d62cea13-8b4e-4666-8ede-a0706b934a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 413 393\n"
     ]
    }
   ],
   "source": [
    "# Use spellchecker to validate words\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "# Find words not in the dictionary\n",
    "spell_set_1 = {word for word in test_words_1 if word not in spell}\n",
    "spell_set_2 = {word for word in test_words_2 if word not in spell}\n",
    "spell_set_3 = {word for word in test_words_3 if word not in spell}\n",
    "\n",
    "print(len(spell_set_1), len(spell_set_2), len(spell_set_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98cf4990-da40-4c90-a739-04d89d0cec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors found with punctuation removed only: 499\n",
      "errors found with punctuation removed after replacing hyphens: 413\n",
      "errors found with case lowered too:  393\n",
      "errors eliminated by removing hyphens:  102\n",
      "errors introduced by removing hyphens:  16\n",
      "errors eliminated by lowering case:  245\n",
      "errors introduced by lowering case:  225\n"
     ]
    }
   ],
   "source": [
    "print(f\"errors found with punctuation removed only: {len(spell_set_1)}\")\n",
    "print(f\"errors found with punctuation removed after replacing hyphens: {len(spell_set_2)}\")\n",
    "print(f\"errors found with case lowered too:  {len(spell_set_3)}\")     \n",
    "print(f\"errors eliminated by removing hyphens:  {len(spell_set_1 - spell_set_2)}\")\n",
    "print(f\"errors introduced by removing hyphens:  {len(spell_set_2 - spell_set_1)}\")\n",
    "print(f\"errors eliminated by lowering case:  {len(spell_set_2 - spell_set_3)}\")\n",
    "print(f\"errors introduced by lowering case:  {len(spell_set_3 - spell_set_2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d1166d1-ae38-4e6d-b8e9-e68e847c6496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordinated empting ofhand Karl Heinz pre Semitic martialling vident humouredness un sheetings inwaiting loverStevens co longue\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "oddities = list(spell_set_2 - spell_set_1)\n",
    "print (' '.join(oddities))\n",
    "print(\"\\n\")\n",
    "# display_words_in_context(test_words_2, oddities, True, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e51945-72f1-4f3c-a7db-92110899501e",
   "metadata": {},
   "source": [
    "Select the ones that could be errors and find them in the original text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d559e4c-2a65-4d8f-b94a-1eb6b7b27861",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_oddities = [\"team of ladies\", \"sleights-of-hand\", \"re a nature-lover\", \"have been self-vident\"]\n",
    "#These should appear in the comprehensive search later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca179134-20fe-428d-a3fe-98abfe94d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text_in_context(text, selected_oddities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f3c39-6375-4ddb-95cc-33ba96de9da1",
   "metadata": {},
   "source": [
    "Select the errors and add them to the replacements dictionary.  The rest of the oddities are not spelling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e447ed-757e-4368-b492-582e9a893dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_set_2_lowered = {word.lower() for word in spell_set_2}\n",
    "len(spell_set_2_lowered - spell_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c346be2-8deb-4457-ad08-57674caa52aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820a237-0ea5-4b39-9f57-b77d0797fa72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f5233c-d9c5-496d-a04c-a5b5bc0ed421",
   "metadata": {},
   "source": [
    "Check words that are picked out by spell checker when made lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332081c8-f0e0-4e6a-8117-29413363c2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_oddities = list(spell_set_3 - spell_set_2)\n",
    "print (' '.join(case_oddities))\n",
    "print(\"\\n\")\n",
    "# display_words_in_context(test_words_3, case_oddities, False, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af28b1d-e513-447f-9e84-05dfc47b8c49",
   "metadata": {},
   "source": [
    "Check words that picked out by spell checker with the original case but passed when made lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7a5b1-a102-4812-836c-c600d525cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_case_oddities = list(spell_set_2 - spell_set_3)\n",
    "print (' '.join(inverse_case_oddities))\n",
    "print(\"\\n\")\n",
    "# display_words_in_context(test_words_2, inverse_case_oddities, False, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1faabab-b81d-4c9b-af5f-c0ed087f9c6c",
   "metadata": {},
   "source": [
    "Run text blob, an alternative spell checker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb85e6-96cf-4714-aefc-90762318eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "print(f\"Starting processing at: {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "\n",
    "\n",
    "# Find invalid words\n",
    "blob_set_1 = set()\n",
    "for word in test_words_1:\n",
    "    result = Word(word).spellcheck()\n",
    "    if result[0][1] < 1.0 or result[0][0] != word:\n",
    "        blob_set_1.add(word)\n",
    "\n",
    "blob_set_2 = set()\n",
    "for word in test_words_2:\n",
    "    result = Word(word).spellcheck()\n",
    "    if result[0][1] < 1.0 or result[0][0] != word:\n",
    "        blob_set_2.add(word)\n",
    "\n",
    "blob_set_3 = set()\n",
    "for word in test_words_3:\n",
    "    result = Word(word).spellcheck()\n",
    "    if result[0][1] < 1.0 or result[0][0] != word:\n",
    "        blob_set_3.add(word)\n",
    "        \n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"Completed processing at: {end_time.strftime('%H:%M:%S')}\")\n",
    "time_taken(start_time, end_time)\n",
    "\n",
    "print(len(blob_set_1), len(blob_set_2), len(blob_set_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8762d370-98fe-4d9d-8acb-5e9e5c9397aa",
   "metadata": {},
   "source": [
    "Now look at how many each spell checker finds that the other didn't find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5ca3f-ce13-48b3-9d4b-fb3c41532bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_only_1 = spell_set_1 - blob_set_1\n",
    "spell_only_2 = spell_set_2 - blob_set_2\n",
    "spell_only_3 = spell_set_3 - blob_set_3\n",
    "\n",
    "blob_only_1 = blob_set_1 - spell_set_1\n",
    "blob_only_2 = blob_set_2 - spell_set_2\n",
    "blob_only_3 = blob_set_3 - spell_set_3\n",
    "\n",
    "both_3 = spell_set_3 & blob_set_3\n",
    "either_3 = spell_set_3 | blob_set_3 # this is an OR symbol\n",
    "\n",
    "print(f\"spell / blob 1 have: {len(spell_set_1)} / {len(blob_set_1)}\")\n",
    "print(f\"spell / blob 2 have: {len(spell_set_2)} / {len(blob_set_2)}\")\n",
    "print(f\"spell / blob 3 have: {len(spell_set_3)} / {len(blob_set_3)}\")\n",
    "print(f\"spell unique / blob unique 1 have: {len(spell_only_1)} / {len(blob_only_1)}\")\n",
    "print(f\"spell unique / blob unique 2 have: {len(spell_only_2)} / {len(blob_only_2)}\")\n",
    "print(f\"spell unique / blob unique 3 have: {len(spell_only_3)} / {len(blob_only_3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d56e8c-ea9c-4b9f-b94b-fb070adac2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_words_in_context(test_words_1, spell_only_1, False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebeb79-bc0e-4edf-8928-bf048c82ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_oddities = list(blob_set_2 - blob_set_1)\n",
    "print (' '.join(blob_oddities))\n",
    "print(\"\\n\")\n",
    "# display_words_in_context(test_words_2, oddities, True, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8b220-e1de-4784-88f8-9060ffda4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spell_only_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee446b7-3e9d-43cd-b0f6-7ed4e242c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(' '.join(word for word in both_3))\n",
    "# display_words_in_context(test_words_2, spell_only_2 - spell_only_1, False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6e9f0-c4c1-4ec3-bc7f-617d8a803e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_raw = \"witha bethat selfvident notseek distinguishedand redding civvy ting carryon putout ell arec simplyaccepting gotto thatsall donttake andthe itinvolved istrue tosit tj1e ifi inwaiting seehe allhis lto ohnothing ill1terest het villagesir imvery loc lotyou verypleased docrucially avery friendsand anysuch wer thevery iarrived ican startseeing canonly allshot contemplatin8 infact ot 0f roas agre ofthe dh owardst itsrather ofhand confidentia1 cansee prise loverstevens canbe chainbers lookingback thelikes beyondsuch farradayscircle al1\"\n",
    "errors_raw = errors_raw.split()\n",
    "errors_raw = sorted(errors_raw)\n",
    "print(len(errors_raw))\n",
    "print (errors_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e1fe36-5eda-4f7c-935a-74be091c28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = ['0f', 'agre', 'al1', 'allhis', 'andthe', 'anysuch', 'arec', 'avery', 'be-that', 'beyondsuch', 'canbe', 'canonly', 'cansee', 'carryon', 'Chainbers', 'confidentia1', 'contemplatin8', 'Dh', \"'distinguished',and\", 'docrucially', \"don'ttake\", 'T ell', \"Farraday'scircle\", 'friendsand', 'gotto', 'het', 'Iarrived', 'Ican', \"if'-I\", 'ill1terest', \"I'mvery\", 'Infact', 'istrue', 'it,involved', \"it'srather\", 'loc ked', 'lookingback', 'lot,you', 'lover,Stevens', 'lto', 'not-seek', 'ofthe', 'Oh,nothing', 'ot', 'owardst', 'putout', 'roas ting', 'seehe', 'self-vident', 'simplyaccepting', 'startseeing', \"that'sall\", 'thelikes', 'thevery', 'tJ1e', 'tosit', 'verypleased', 'village,sir', 'wer', 'with-a']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849252c9-d58e-47e8-866e-6b76e280fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82d10c-1a4e-4082-a4c6-d2ab2284c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"distinguishedand\" in errors_raw)\n",
    "print(\"distinguishedand\" in errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f880158-9c22-4145-ac51-934cf1c030d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_tt == errors_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946ebd0-75f3-4083-a3a9-7bc4838484e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = ['of', 'agree', 'all', 'all his', 'and the', 'any such', 'care', 'a very', 'be that', 'beyond such', 'can be', 'can only', 'can see', 'carry on', 'Chambers', 'confidential', 'contemplating', 'Oh', \"'distinguished', and\", 'do crucially', \"don't take\", 'Tell', 'Farradays Circle', 'friends and', 'got to', 'the', 'I arrived', 'I can', 'if I', 'interest', \"I'm very\", 'In fact', 'is true', 'it involved', \"its rather\", 'locked', 'looking back', 'lot, you', 'lover, Stevens', 'to', 'not seek', 'of the', 'Oh, nothing', 'to', 'towards', 'put out', 'roasting', 'see he', 'self-evident', 'simply accepting', 'start seeing', \"that's all\", 'the likes', 'the very', 'the', 'to sit', 'very pleased', 'village, sir', 'were', 'with a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85f2e9-3c4e-42b8-bef6-115f9bde36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_1 = set(errors_tt)\n",
    "set_2 = set(errors)\n",
    "print(len(set_1), len(set_2))\n",
    "print(set_1 - set_2)\n",
    "print(set_2 - set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675990c1-ceac-44b5-8265-475c9c1225ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_spell_1 = 0\n",
    "counter_spell_2 = 0\n",
    "counter_spell_3 = 0\n",
    "counter_blob_1 = 0\n",
    "counter_blob_2 = 0\n",
    "counter_blob_3 = 0\n",
    "\n",
    "for word in errors_tt:\n",
    "    if word in spell_set_1: counter_spell_1 += 1 \n",
    "    if word in spell_set_2: counter_spell_2 += 1\n",
    "    if word in spell_set_3: counter_spell_3 += 1\n",
    "    if word in blob_set_1: counter_blob_1 += 1\n",
    "    if word in blob_set_2: counter_blob_2 += 1\n",
    "    if word in blob_set_3: counter_blob_3 += 1\n",
    "print (counter_spell_1, counter_spell_2, counter_spell_3)\n",
    "print (counter_blob_1, counter_blob_2, counter_blob_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5585b2c0-381e-430d-ae1c-9ecd431b7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in errors_tt:\n",
    "    if word not in blob_set_3: print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ce725f-7094-4cba-9323-19ae8198fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_spell_1 = 0\n",
    "counter_spell_2 = 0\n",
    "counter_spell_3 = 0\n",
    "counter_blob_1 = 0\n",
    "counter_blob_2 = 0\n",
    "counter_blob_3 = 0\n",
    "\n",
    "for word in errors:\n",
    "    if word in spell_set_1: counter_spell_1 += 1 \n",
    "    if word in spell_set_2: counter_spell_2 += 1\n",
    "    if word in spell_set_3: counter_spell_3 += 1\n",
    "    if word in blob_set_1: counter_blob_1 += 1\n",
    "    if word in blob_set_2: counter_blob_2 += 1\n",
    "    if word in blob_set_3: counter_blob_3 += 1\n",
    "print (counter_spell_1, counter_spell_2, counter_spell_3)\n",
    "print (counter_blob_1, counter_blob_2, counter_blob_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5700764-b25f-4aa7-a2da-a01a80437a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(errors_raw), len(errors), len(corrections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d57016-1f5a-4d20-9802-ee8a471831ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_spell_1 = 0\n",
    "counter_spell_2 = 0\n",
    "counter_spell_3 = 0\n",
    "counter_blob_1 = 0\n",
    "counter_blob_2 = 0\n",
    "counter_blob_3 = 0\n",
    "counter_both_3 = 0\n",
    "\n",
    "for word in errors_raw:\n",
    "    if word in spell_set_1: counter_spell_1 += 1 \n",
    "    if word in spell_set_2: counter_spell_2 += 1\n",
    "    if word in spell_set_3: counter_spell_3 += 1\n",
    "    else: print(\"not in spell_3: \", word)\n",
    "    if word in blob_set_1: counter_blob_1 += 1\n",
    "    if word in blob_set_2: counter_blob_2 += 1\n",
    "    if word in blob_set_3: counter_blob_3 += 1\n",
    "    else: print(\"not in blob_3: \", word)\n",
    "    if word in both_3: counter_both_3 += 1\n",
    "    else: print(\"not in both_3: \", word)\n",
    "print (counter_spell_1, counter_spell_2, counter_spell_3)\n",
    "print (counter_blob_1, counter_blob_2, counter_blob_3)\n",
    "print(counter_both_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e3787-ea20-41dc-9f62-9c52bac4de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corrections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb6328-63c2-4fdb-a2aa-035c0c21aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for error, correction in zip(errors, corrections):\n",
    "    if error == correction:\n",
    "        display_words_in_context(test_words_3, [error], False, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b90b91-cb83-4a82-8cfd-7530e5b1e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in errors:\n",
    "    if word not in text:\n",
    "        display_words_in_context(test_words_3, [word], False, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319fa11-3b46-49b0-aa44-fd3cb74b62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text_in_context(text, [\"never forgive me\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bdd1cb-bb9b-46b8-ad5a-0f59c55dd11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([word in blob_set_3 for word in errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b3ecf-0cb8-4c0f-a405-964c1627d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "where_is(\"bethat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b4c56-a27d-4a8b-9e7a-818b58ea0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"be that\" in corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e443ecc-2096-4217-8b15-74f54e935e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"allshot\" in both_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ac2a16-7bd1-4897-ba5a-7577905fa458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d72464-2574-40df-9f36-ffb4b7a10957",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text_in_context(text,[\"ell\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472945e-1b91-4232-b84f-b0475f868997",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {error: correction for error, correction in zip (errors, corrections)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee09f4-ed5a-4c5f-b4af-69068856abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(replacements[\"with-a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec50fa-f189-4614-9c82-7ae26dfffef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(word for word in (blob_only_2 - blob_only_1)))\n",
    "display_words_in_context(test_words_2, blob_only_2 - blob_only_1, False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce6245-245f-4485-a247-9c51944a1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(word for word in (spell_only_3 - spell_only_2 - spell_only_1)))\n",
    "display_words_in_context(test_words_3, spell_only_3 - spell_only_2 - spell_only_1, False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a20f68-d116-4d08-8742-47a8861c7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(word for word in (blob_only_3 - blob_only_2 - blob_only_1)))\n",
    "display_words_in_context(test_words_3, blob_only_3 - blob_only_2 - blob_only_1, False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c5c1a-765d-4437-9779-ad2e660470f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4cf3b-7ae6-4e23-a7a0-31697822a7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6279f-1709-454e-ac5b-7ac43b78c086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57df1d8-6fcc-4357-9728-f5db60e255c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec8f3a-dde0-4ada-8a9a-53d314943e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd26866-fded-4a8d-bdad-6a0fb5331212",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for word in blob_set_2:\n",
    "    if word.lower() in blob_set_3:\n",
    "        print(f\"{word} in both\")\n",
    "        print(Word(word).spellcheck())\n",
    "        if word != word.lower():\n",
    "            print(Word(word.lower()).spellcheck())\n",
    "    else:\n",
    "        print (f\"{word} in 2 but not in 3\")\n",
    "        print(Word(word).spellcheck())\n",
    "    print(\"\\n\")    \n",
    "    counter = counter + 1\n",
    "    if counter > 15: break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed2fbb-f5a0-4bb1-b393-16de31da9166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da527b-568f-4896-8e01-68b7ed48a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = \"istrue canbe im tj1e overath laval itsrather wer silverss gotto thelikes ifi 0f prise ritz ofthe andthe lto lookingback al1 verypleased het thevery agre bethat seehe ot selfvident sleightsofhand cansee thatsall distinguishedand villagesir docrucially arec farradayscircle witha canonly barnets redding tosit clementss newtmating ill1terest anysuch dh lewiss donttake allhis contemplatin8 startseeing symons owardst imvery simplyaccepting selftraining wellcontented evercourteous donttake allhis lastminute civvy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0454a-fb4a-4054-94ac-24de224ee2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d7a7e-1d51-498e-ab10-07a65049dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words=[\"ohnothing\", \"iarrived\", \"friendsand\", \"allshot\"]\n",
    "for word in bad_words:\n",
    "    print(word in errors_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92cadc7-565c-497d-8a01-72d61ed4fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions_split = suggestions.split(' ')\n",
    "suggestions_sorted = sorted(suggestions_split)\n",
    "print(suggestions_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09427cc1-f405-4464-b6be-7fc259bd2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_sorted = sorted(errors_split)\n",
    "print (errors_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d103da-705d-42d8-8f44-317d466de1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in bad_words:\n",
    "    errors_split.append(word)\n",
    "print(errors_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65371496-ddd5-4dc9-b53e-54a08b580303",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff9a19-380e-4acc-9650-a975353dca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_split = ['istrue', 'canbe', 'im', 'tj1e', 'overath', 'laval', 'itsrather', 'wer', 'silverss', 'gotto', 'thelikes', 'ifi', '0f', 'prise', 'ritz', 'ofthe', 'andthe', 'lto', 'lookingback', 'al1', 'verypleased', 'het', 'thevery', 'agre', 'bethat', 'seehe', 'ot', 'selfvident', 'sleightsofhand', 'cansee', 'thatsall', 'distinguishedand', 'villagesir', 'docrucially', 'arec', 'farradayscircle', 'witha', 'canonly', 'barnets', 'redding', 'tosit', 'clementss', 'newtmating', 'ill1terest', 'anysuch', 'dh', 'lewiss', 'donttake', 'allhis', 'contemplatin8', 'startseeing', 'symons', 'owardst', 'imvery', 'simplyaccepting', 'selftraining', 'wellcontented', 'evercourteous', 'donttake', 'allhis', 'lastminute', 'civvy', 'ohnothing', 'iarrived', 'friendsand', 'allshot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f858ade-6194-4520-9ae3-a0230103701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for bad_word in bad_words:\n",
    "    ind = text.find(bad_word)\n",
    "    if ind == -1:\n",
    "        print(f\"{bad_word} not found\")\n",
    "    else:\n",
    "        window = text[ind-50:ind+50]\n",
    "        window = window.replace(\"\\n\", \" \")\n",
    "        window = window.replace(\"\\t\", \" \")\n",
    "        print(window)\n",
    "    #test_result = (Word(test_word).spellcheck())\n",
    "    #print(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a541d2-64e9-4cdf-8cfc-c1a076ff7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = text.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "bad_words=[\"ohnothing\", \"iarrived\", \"friendsand\", \"allshot\"]\n",
    "bad_words = [\"ell\", \"ting\"]\n",
    "for bad_word in bad_words:\n",
    "    try:\n",
    "        ind = test_words.index(bad_word)\n",
    "        print(f\"The index of '{bad_word}' is: {ind}\")\n",
    "        print(' '.join(test_words[ind - 10:ind + 10]))\n",
    "    except ValueError:\n",
    "        print(f\"'{item_to_find}' not found in the list.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0c707-02c7-4573-b08d-bba3262278ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(blob_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b8c3c-6402-404f-8e3a-267bacf42033",
   "metadata": {},
   "outputs": [],
   "source": [
    "both = spell_set & blob_set\n",
    "print(len(both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de333e45-7126-47bb-9447-b42f1ece6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(word for word in both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f25e8a-925c-417d-a81f-bd1ca37c6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_errors = \"istrue canbe im tj1e overath laval itsrather wer silverss gotto thelikes ifi 0f prise ritz ofthe andthe lto lookingback al1 verypleased het thevery agre bethat seehe ot selfvident sleightsofhand cansee thatsall distinguishedand villagesir docrucially arec farradayscircle witha canonly barnets redding tosit clementss newtmating ill1terest anysuch dh lewiss donttake allhis contemplatin8 startseeing symons owardst imvery simplyaccepting selftraining wellcontented evercourteous donttake allhis lastminute civvy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9ea58-49af-4e8f-9be7-779830d8eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_errors_split = set(original_errors.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546bee2-8dbc-4329-b1bb-892c2e7bc980",
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = {word for word in original_errors_split if word not in errors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c805622-08b7-4460-ba37-84e56269439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ([(word, word in blob_set_3) for word in missed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab8b65-c34e-4b31-9649-038e934bfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ([(word, word in both_3) for word in missed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97c339-224d-428e-a557-ec3b9d6be1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ([(word, word in spell_set_3) for word in missed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ab175-8e0c-45ad-9513-1d67321fe30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in errors_split:\n",
    "    if word not in text:\n",
    "        if word not in cleaned_suggestions:\n",
    "            print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdea9a-50b8-49a3-846a-597e6c2e4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "where_is(\"distinguishedand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbc744-436d-42d4-af84-d7197cadbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([word for word in blob_set_3 if word not in blob_only])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655e006-5bfb-4dbf-808d-141d92766b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions = \"Overath Laval 1e with-a it'srather Silvers's Farraday'scircle Barnet's Clements's newt-mating Lewis's don'ttake I'mvery self-training well-contented ever-courteous don'ttake last-minute Ritz Redding self-vident sleights-of-hand be-that that'sall d-h Symons\"\n",
    "cleaned_suggestions = suggestions.translate(str.maketrans('', '', string.punctuation)).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8efe08-4c67-4fc7-9d9f-b34d928f9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in suggestions.split(' '):\n",
    "    if word not in text:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8498acb-31a9-4857-b72c-f83224304ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in errors_split:\n",
    "    if word not in cleaned_suggestions:`\n",
    "        if word not in test_words:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946ef07-9113-43ca-8aaf-d9a743a9b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" prise \" in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce07e4-5c13-4648-8f81-b3401fc9beb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
