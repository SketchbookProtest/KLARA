{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4582d0-707e-4999-afd0-e99d61c6b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza, re, benepar, psutil, gc, json, csv, time\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from textstat import textstat\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_infix_regex\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import Tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import LogLocator, LogFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45141226-b70b-430d-b0dd-6bbe94c47f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These only need to run once\n",
    "# stanza.download('en')\n",
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "# nltk.download('punkt_tab')\n",
    "# benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dec98a2-4914-423a-8a2d-e1dffd7d78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memcheck():\n",
    "    gc.collect()\n",
    "    memory_info = psutil.virtual_memory()\n",
    "\n",
    "    # Display the memory information in GB\n",
    "    total_memory = memory_info.total / (1024 ** 3)\n",
    "    available_memory = memory_info.available / (1024 ** 3)\n",
    "    used_memory = memory_info.used / (1024 ** 3)\n",
    "\n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "    print(f\"Used Memory: {used_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc2688-7e7b-4d93-a8bf-6d708f51d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_richness(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    fdist = FreqDist(tokens)\n",
    "    type_token_ratio = len(fdist) / len(tokens)\n",
    "    return type_token_ratio\n",
    "\n",
    "# test it\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "richness = lexical_richness(test_text)\n",
    "print(f\"Lexical Richness (Type-Token Ratio): {richness}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3532c22-a8b6-4468-80d9-adaec0aaae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generator(text):\n",
    "    \"\"\"\n",
    "    Generates sentences from a large text string one at a time.\n",
    "\n",
    "    :param text: The large text string\n",
    "    :yield: Each sentence as a stanza Sentence object\n",
    "    \"\"\"\n",
    "    buffer = \"\"\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        buffer += line.strip() + \" \"\n",
    "        doc_buffer = nlp_tokens(buffer)\n",
    "        \n",
    "        for sentence in doc_buffer.sentences:\n",
    "            yield sentence\n",
    "            \n",
    "        # Clear the buffer if it's been processed\n",
    "        if doc_buffer.sentences:\n",
    "            buffer = \"\"\n",
    "\n",
    "    # Process any remaining sentences in the buffer\n",
    "    if buffer.strip():\n",
    "        doc_buffer = nlp_tokens(buffer)\n",
    "        for sentence in doc_buffer.sentences:\n",
    "            yield sentence\n",
    "\n",
    "def count_sentences_in_string(text):\n",
    "    \"\"\"\n",
    "    Counts the number of sentences in a large text string using a generator.\n",
    "\n",
    "    :param text: The large text string\n",
    "    :return: Total number of sentences in the text\n",
    "    \"\"\"\n",
    "    sentence_count = sum(1 for _ in sentence_generator(text))\n",
    "    return sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f596525-00eb-484a-9e4b-fd259edcc238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_sentence_graph(sentence):\n",
    "    # plots graph of a stanza sentence object\n",
    "    # Initialize a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add nodes and edges from the dependency parse\n",
    "    for word in sentence.words:\n",
    "        G.add_node(word.id, label=word.text)\n",
    "        if word.head != 0:  # If the word has a head, add an edge\n",
    "            G.add_edge(word.head, word.id, label=word.deprel)\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.spring_layout(G)  # Positioning of nodes\n",
    "    labels = nx.get_node_attributes(G, 'label')\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    nx.draw(G, pos, labels=labels, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrows=True)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36393888-8e91-4cf4-b1cf-0f3ad6493ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_ease(text):\n",
    "\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade = textstat.flesch_kincaid_grade(text)\n",
    "    gunning_fog = textstat.gunning_fog(text)\n",
    "    coleman_liau_index = textstat.coleman_liau_index(text)\n",
    "\n",
    "    print(f\"Flesch Reading Ease: {flesch_reading_ease}\")\n",
    "    print(f\"Flesch-Kincaid Grade Level: {flesch_kincaid_grade}\")\n",
    "    print(f\"Gunning Fog Index: {gunning_fog}\")\n",
    "    print(f\"Coleman-Liau Index: {coleman_liau_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a37dd8-c7f3-45b3-916f-4aefc6b8327a",
   "metadata": {},
   "source": [
    "nlp files to be created (as needed) every time the notebook is launched (or the kernel is restarted):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d39aac-5e0b-435b-946d-253a64455364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 12:14:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc9e64798124766ac8f5daec7f81d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 12:14:25 INFO: Downloaded file to C:\\Users\\Roland\\stanza_resources\\resources.json\n",
      "2024-08-31 12:14:26 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| pos       | combined_charlm           |\n",
      "| lemma     | combined_nocharlm         |\n",
      "| depparse  | combined_charlm           |\n",
      "| sentiment | sstplus_charlm            |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-08-31 12:14:26 INFO: Using device: cpu\n",
      "2024-08-31 12:14:26 INFO: Loading: tokenize\n",
      "2024-08-31 12:14:28 INFO: Loading: mwt\n",
      "2024-08-31 12:14:28 INFO: Loading: pos\n",
      "2024-08-31 12:14:28 INFO: Loading: lemma\n",
      "2024-08-31 12:14:28 INFO: Loading: depparse\n",
      "2024-08-31 12:14:28 INFO: Loading: sentiment\n",
      "2024-08-31 12:14:29 INFO: Loading: ner\n",
      "2024-08-31 12:14:29 INFO: Done loading processors!\n",
      "2024-08-31 12:14:29 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0494ceaac5145ed89c6e92ff17bb7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 12:14:30 INFO: Downloaded file to C:\\Users\\Roland\\stanza_resources\\resources.json\n",
      "2024-08-31 12:14:30 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-08-31 12:14:30 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2024-08-31 12:14:30 INFO: Using device: cpu\n",
      "2024-08-31 12:14:30 INFO: Loading: tokenize\n",
      "2024-08-31 12:14:30 INFO: Loading: mwt\n",
      "2024-08-31 12:14:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse,sentiment,ner')\n",
    "nlp_tokens = stanza.Pipeline(lang='en', processors='tokenize', use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b08563ff-956e-4d86-9f48-a8c3a6103518",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add the benepar parser to the pipeline\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained('t5-small', clean_up_tokenization_spaces=True)  \n",
    "# default is currently True but will change to False\n",
    "\n",
    "#nlp_spacy.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0d0beb-8f5d-4728-aa31-6e6838f849f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Roland/Documents/AI/stylometry/Kazuo Ishiguro - The Remains of the Day.txt\n"
     ]
    }
   ],
   "source": [
    "source_texts=[\"Kazuo Ishiguro - Never Let Me Go.txt\", \"Kazuo Ishiguro - The Remains of the Day\"]\n",
    "directory_path = \"C:/Users/Roland/Documents/AI/stylometry/\"\n",
    "file_path = directory_path+source_texts[1]+\".txt\"\n",
    "print (file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd6113d-d26e-40d0-8b3a-6a943f92db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read with encoding: utf-8\n",
      "Failed to read with encoding: ascii\n",
      "Successfully read the file with encoding: latin-1 (encoding index 2).  This gives the beginning as:\n",
      "    Prologue: July 1956    Darlington Hall    IT seems increasingly likely that I really will undertake the expedition that has been preoccupying my imagination now for some days. An expedition, I should sa\n",
      "\n",
      "Successfully read the file with encoding: windows-1252 (encoding index 3).  This gives the beginning as:\n",
      "    Prologue: July 1956    Darlington Hall    IT seems increasingly likely that I really will undertake the expedition that has been preoccupying my imagination now for some days. An expedition, I should sa\n",
      "\n",
      "Successfully read the file with encoding: iso-8859-1 (encoding index 4).  This gives the beginning as:\n",
      "    Prologue: July 1956    Darlington Hall    IT seems increasingly likely that I really will undertake the expedition that has been preoccupying my imagination now for some days. An expedition, I should sa\n",
      "\n",
      "Successfully read the file with encoding: iso-8859-15 (encoding index 5).  This gives the beginning as:\n",
      "    Prologue: July 1956    Darlington Hall    IT seems increasingly likely that I really will undertake the expedition that has been preoccupying my imagination now for some days. An expedition, I should sa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encodings = ['utf-8', 'ascii', 'latin-1','windows-1252', 'iso-8859-1', 'iso-8859-15']\n",
    "for i, encoding in enumerate(encodings):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as file:\n",
    "            test_text = file.read()\n",
    "        print(\"\"\"Successfully read the file with encoding: {encoding} (encoding index {ind}).  This gives the beginning as:\\n{content}\\n\"\"\".format(encoding=encoding, ind=i, content=test_text[0:200].replace(\"\\n\", \"  \")))    \n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read with encoding: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc67f44-b4e9-45e0-b3a0-4ceaed1b471c",
   "metadata": {},
   "source": [
    "Now load text file with selected encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "688deb8d-782e-4c40-a9bb-70a3e4554f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = encodings[2]\n",
    "with open(file_path, 'r', encoding=encoding) as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f842fe2-dfce-4e07-ba59-89d3415636ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in book:  423697\n",
      "Total number of sentences: 4133\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total characters in book:  {len(text)}\")\n",
    "sentence_count = count_sentences_in_string(text)\n",
    "print(f\"Total number of sentences: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "897676c6-57ae-4edb-88c1-c7ba7825300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing at: 12:15:48\n",
      "Total Memory: 127.91 GB\n",
      "Available Memory: 99.10 GB\n",
      "Used Memory: 28.82 GB\n",
      "Total Memory: 127.91 GB\n",
      "Available Memory: 98.85 GB\n",
      "Used Memory: 29.06 GB\n",
      "Completed parsing at: 12:24:32\n",
      "Time Taken: 0 hours, 8 minutes, 43 seconds\n",
      "\n",
      "doc_stanza has 4132 sentences.\n"
     ]
    }
   ],
   "source": [
    "# process the text with stanza \n",
    "\n",
    "# Record the start time\n",
    "start_time = datetime.now()\n",
    "print(f\"Starting processing at: {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "memcheck()\n",
    "doc_stanza = nlp_stanza(text)\n",
    "memcheck()\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"Completed parsing at: {end_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Print the time taken in hours, minutes, and seconds\n",
    "hours, remainder = divmod(time_taken.seconds, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(f\"Time Taken: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "\n",
    "print(f\"\\ndoc_stanza has {len(doc_stanza.sentences)} sentences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc70e5-5de8-4095-98ae-90b2d3adec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc_stanza\n",
    "lemmas_with_punct = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
    "lemmas = [word.lemma for sentence in doc.sentences for word in sentence.words if word.upos != 'PUNCT']\n",
    "print(len(lemmas))\n",
    "lemma_counts = Counter(lemmas)\n",
    "lemma_list = list(lemma_counts.items())\n",
    "sorted_lemma_list = sorted(lemma_list, key=lambda x: x[1], reverse=True)\n",
    "print(sorted_lemma_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "233c0f6b-d2be-45db-9f31-e3fe341d66f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['on', 'see', 'my', 'person', 'he', 'take', 'the', 'opportunity', 'to', 'inform', 'I', 'that', 'he', 'have', 'just', 'that', 'moment', 'finalize', 'plan', 'to', 'return', 'to', 'the', '<GPE>', '<GPE>', 'for', 'a', 'period', 'of', '<DATE>', '<DATE>', 'between', '<DATE>', 'and', '<DATE>', 'bring', 'I', 'the', 'head', 'of', 'John', 'the', 'Baptist', 'say', 'King', 'Charles']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example text\n",
    "test_text = \"On seeing my person, he took the opportunity to inform me that he had just that moment finalized plans to return to the United States for a period of five weeks between August and September. 'Bring me the head of John the Baptist, said King Charles'\"\n",
    "# Process the text\n",
    "test_doc = nlp_stanza(test_text)\n",
    "\n",
    "# Create a list to hold the modified lemmas with placeholders\n",
    "lemmas_with_placeholders = []\n",
    "\n",
    "# Iterate over sentences\n",
    "for sentence in test_doc.sentences:\n",
    "    # Create a set of named entity spans to avoid overlapping replacements\n",
    "    entity_spans = [(entity.start_char, entity.end_char, entity.type) for entity in sentence.ents]\n",
    "    \n",
    "    # Initialize the last position\n",
    "    last_pos = 0\n",
    "\n",
    "    # Iterate over tokens in the sentence\n",
    "    for token in sentence.tokens:\n",
    "        for word in token.words:\n",
    "            start_char = sentence.text.index(word.text)\n",
    "            end_char = start_char + len(token.text)\n",
    "\n",
    "            # Replace named entities with placeholders\n",
    "            is_entity = False\n",
    "            for start, end, ent_type in entity_spans:\n",
    "                if start_char >= start and end_char <= end:\n",
    "                    placeholder = f\"<{ent_type}>\"\n",
    "                    if not is_entity:\n",
    "                        lemmas_with_placeholders.append(placeholder)\n",
    "                        is_entity = True\n",
    "                    break\n",
    "\n",
    "            if not is_entity:\n",
    "                # Append token lemma if it’s not part of a named entity and not punctuation\n",
    "                if token.words[0].pos != 'PUNCT':  # Check POS tag for punctuation\n",
    "                    lemmas_with_placeholders.append(token.words[0].lemma)\n",
    "\n",
    "# Print the modified lemmas with placeholders\n",
    "print(lemmas_with_placeholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9dc132-1519-4569-af45-de15a755640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Generate lemma list including punctuation\n",
    "lemmas_with_punctuation = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "# Generate bigrams excluding those with punctuation\n",
    "bigrams = [\n",
    "    (lemmas_with_punctuation[i], lemmas_with_punctuation[i+1])\n",
    "    for i in range(len(lemmas_with_punctuation) - 1)\n",
    "    if lemmas_with_punctuation[i] not in string.punctuation and lemmas_with_punctuation[i+1] not in string.punctuation\n",
    "]\n",
    "\n",
    "# Count the occurrences of each lemma\n",
    "from collections import Counter\n",
    "lemma_counts = Counter(lemmas_with_punctuation)\n",
    "\n",
    "# Count the occurrences of each bigram\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# Sort the lemma counts by frequency in descending order\n",
    "sorted_lemma_tuples = sorted(lemma_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort the bigram counts by frequency in descending order\n",
    "sorted_bigram_tuples = sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted lemma and bigram counts\n",
    "print(\"Sorted Lemma Frequencies:\")\n",
    "print(sorted_lemma_tuples)\n",
    "\n",
    "print(\"\\nSorted Bigram Frequencies:\")\n",
    "print(sorted_bigram_tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c600b39-48a3-4a33-ab91-743c007bda58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in test_doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.pos != word.upos: print(f\"Word: {word.text}, UPOS: {word.upos}, XPOS: {word.xpos}, POS: {word.pos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa317320-0fc6-4eb3-b8d9-019e8b5915bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the text with spacy\n",
    "\n",
    "# Record the start time\n",
    "start_time = datetime.now()\n",
    "print(f\"Starting processing at: {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "memcheck()\n",
    "doc_spacy = nlp_spacy(\"We know you can hear us. This is the voice of the mysterons.\")\n",
    "memcheck()\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"Completed parsing at: {end_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Print the time taken in hours, minutes, and seconds\n",
    "hours, remainder = divmod(time_taken.seconds, 3600)\n",
    "minutes, seconds = divmod(remainder, 60)\n",
    "print(f\"Time Taken: {hours} hours, {minutes} minutes, {seconds} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50618733-5cf1-464a-85a5-250c6c788955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sentence in the spacy document\n",
    "sent = list(doc_spacy.sents)[1]\n",
    "\n",
    "# Convert the constituency parse to an nltk Tree\n",
    "tree = Tree.fromstring(sent._.parse_string)\n",
    "\n",
    "# Plot the tree\n",
    "tree.pretty_print()\n",
    "\n",
    "# Optionally, draw the tree using matplotlib\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232c51d-49a7-45a5-b2a2-7e304799eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the second sentence in the spacy document\n",
    "sent = list(doc_spacy.sents)[1]\n",
    "\n",
    "# Print the parse string to check its structure\n",
    "print(sent._.parse_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d9cf576-84d0-4d59-a4ae-f40977d076f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the United States\n",
      "GPE\n",
      "five weeks\n",
      "DATE\n",
      "August\n",
      "DATE\n",
      "September\n",
      "DATE\n"
     ]
    }
   ],
   "source": [
    "#attributes = [attr for attr in dir(test_doc.sentences[0].entities) if not callable(getattr(test_doc.sentences[0].entities, attr))]\n",
    "#print(attributes)\n",
    "for entity in test_doc.sentences[0].entities:\n",
    "    print(entity.text)\n",
    "    print(entity.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7149518-46c0-4a48-aa1e-ae66ce8356af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = test_doc.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84324485-f30a-4a55-9691-ccb7085f562d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_comments', '_constituency', '_dependencies', '_doc', '_doc_id', '_empty_words', '_enhanced_dependencies', '_ents', '_index', '_process_tokens', '_sent_id', '_sentiment', '_text', '_tokens', '_words', 'add_comment', 'add_property', 'build_dependencies', 'build_ents', 'build_fake_dependencies', 'comments', 'constituency', 'dependencies', 'dependencies_string', 'doc', 'doc_id', 'empty_words', 'entities', 'ents', 'has_enhanced_dependencies', 'id', 'index', 'print_dependencies', 'print_tokens', 'print_words', 'rebuild_dependencies', 'sent_id', 'sentiment', 'text', 'to_dict', 'tokens', 'tokens_string', 'words', 'words_string']\n"
     ]
    }
   ],
   "source": [
    "print(dir(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa3d6889-bccd-46df-94f4-69520fa65698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_coref_chains',\n",
       " '_deprel',\n",
       " '_end_char',\n",
       " '_feats',\n",
       " '_head',\n",
       " '_id',\n",
       " '_is_null',\n",
       " '_lemma',\n",
       " '_mexp',\n",
       " '_misc',\n",
       " '_parent',\n",
       " '_sent',\n",
       " '_start_char',\n",
       " '_text',\n",
       " '_upos',\n",
       " '_xpos',\n",
       " 'add_property',\n",
       " 'coref_chains',\n",
       " 'deprel',\n",
       " 'deps',\n",
       " 'end_char',\n",
       " 'feats',\n",
       " 'head',\n",
       " 'id',\n",
       " 'lemma',\n",
       " 'manual_expansion',\n",
       " 'misc',\n",
       " 'parent',\n",
       " 'pos',\n",
       " 'pretty_print',\n",
       " 'sent',\n",
       " 'start_char',\n",
       " 'text',\n",
       " 'to_conll_text',\n",
       " 'to_dict',\n",
       " 'upos',\n",
       " 'xpos']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = sentence.words[0]\n",
    "dir(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d779c50-9b8c-47f0-a133-fec3eaf4285a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "print(dir(word.lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f866bbb8-53d7-4aeb-93d1-9ce66687d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_end_char', '_id', '_is_null', '_mexp', '_misc', '_multi_ner', '_ner', '_sent', '_spaces_after', '_spaces_before', '_start_char', '_text', '_words', 'add_property', 'consolidate_whitespace', 'end_char', 'id', 'is_mwt', 'manual_expansion', 'misc', 'multi_ner', 'ner', 'pretty_print', 'sent', 'spaces_after', 'spaces_before', 'start_char', 'text', 'to_conll_text', 'to_dict', 'words']\n"
     ]
    }
   ],
   "source": [
    "token = sentence.tokens[0]\n",
    "print(dir(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "196e29e4-e423-4890-a564-eebc5dba7684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On\n",
      "[{\n",
      "  \"id\": 1,\n",
      "  \"text\": \"On\",\n",
      "  \"lemma\": \"on\",\n",
      "  \"upos\": \"SCONJ\",\n",
      "  \"xpos\": \"IN\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"mark\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 2\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "print(token.text)\n",
    "\n",
    "print (token.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afa79c-6d6f-40b8-8ed9-fd7e6373b868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9147aa-645d-4459-ac91-ac776ce14729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa31ab7-4a92-4df6-a55f-ba93f87c8391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13358f34-aa2c-4b13-8045-50cbe302c875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ca853e2-ed0e-4c67-92da-80ab5c367e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_end_char', '_id', '_is_null', '_mexp', '_misc', '_multi_ner', '_ner', '_sent', '_spaces_after', '_spaces_before', '_start_char', '_text', '_words', 'add_property', 'consolidate_whitespace', 'end_char', 'id', 'is_mwt', 'manual_expansion', 'misc', 'multi_ner', 'ner', 'pretty_print', 'sent', 'spaces_after', 'spaces_before', 'start_char', 'text', 'to_conll_text', 'to_dict', 'words']\n"
     ]
    }
   ],
   "source": [
    "print(dir(test_doc.sentences[0].tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d58347ec-9e55-45d8-8715-f76bd3fa0c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on O\n",
      "see O\n",
      "my O\n",
      "person O\n",
      ", O\n",
      "he O\n",
      "take O\n",
      "the O\n",
      "opportunity O\n",
      "to O\n",
      "inform O\n",
      "I O\n",
      "that O\n",
      "he O\n",
      "have O\n",
      "just O\n",
      "that O\n",
      "moment O\n",
      "finalize O\n",
      "plan O\n",
      "to O\n",
      "return O\n",
      "to O\n",
      "the B-GPE\n",
      "United I-GPE\n",
      "State E-GPE\n",
      "for O\n",
      "a O\n",
      "period O\n",
      "of O\n",
      "five B-DATE\n",
      "week E-DATE\n",
      "between O\n",
      "August S-DATE\n",
      "and O\n",
      "September S-DATE\n",
      ". O\n",
      "' O\n",
      "bring O\n",
      "I O\n",
      "the O\n",
      "head O\n",
      "of O\n",
      "John B-PERSON\n",
      "the I-PERSON\n",
      "Baptist E-PERSON\n",
      ", O\n",
      "say O\n",
      "King O\n",
      "Charles S-PERSON\n",
      "' O\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_doc.sentences:\n",
    "    for token in sentence.tokens:\n",
    "        print(token.words[0].lemma, token.ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1422ac30-6989-4f0e-a3dd-6c06721629d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labels = set()\n",
    "for sentence in doc_stanza.sentences:\n",
    "    for word in sentence.words:\n",
    "        ner_labels.add(word.ner)\n",
    "print(len(ner_labels))\n",
    "#print(\"NER Labels used:\", ner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd05c9d-f1dc-4393-81be-20d1b0b5286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sentences[:10]:\n",
    "    print(f\"Sentence: {sentence.text}\")\n",
    "    for entity in sentence.ents:\n",
    "        print(f\"Entity: {entity.text}, Type: {entity.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656cd940-e60c-4942-b174-658e9e349273",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.sentences[0].words[0].ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8eb7c8-c000-4925-aad9-daf9556ed230",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemmas)\n",
    "print(lemmas[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7517ce-1ff2-4040-8041-20468cddc071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of verbs in each sentence\n",
    "verb_counts = []\n",
    "for sentence in doc.sentences:\n",
    "    verb_count = sum(1 for word in sentence.words if word.pos == 'VERB')\n",
    "    verb_counts.append(verb_count)\n",
    "\n",
    "# Plotting the histogram\n",
    "plt.hist(verb_counts, bins=range(1, max(verb_counts) + 2), edgecolor='black', align='left')\n",
    "plt.title('Histogram of the Number of Verbs per Sentence')\n",
    "plt.xlabel('Number of Verbs')\n",
    "plt.ylabel('Number of Sentences')\n",
    "plt.xticks(range(1, max(verb_counts) + 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9d278-d756-4958-80fb-f7b9b248096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example: sorted_lemma_tuples = [('lemma1', count1), ('lemma2', count2), ...]\n",
    "# Ensure `sorted_lemma_tuples` is sorted by the count in descending order.\n",
    "\n",
    "# Extract lemmas and their counts\n",
    "lemmas = [lemma for lemma, count in sorted_lemma_list]\n",
    "counts = [count for lemma, count in sorted_lemma_list]\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(12, 8))  # Increase figure size if necessary\n",
    "plt.bar(range(len(lemmas)), counts, color='lightcoral')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title('Lemma Frequency Distribution (Log Scale)')\n",
    "plt.xlabel('Lemmas')\n",
    "plt.ylabel('Counts (Log Scale)')\n",
    "plt.yscale('log')  # Set y-axis to logarithmic scale\n",
    "\n",
    "# Customizing the y-axis ticks\n",
    "plt.gca().yaxis.set_major_locator(LogLocator(base=10.0, numticks=10))  # Set ticks at 10^0, 10^1, 10^2, etc.\n",
    "plt.gca().yaxis.set_major_formatter(LogFormatter(base=10.0, labelOnlyBase=False))  # Use base-10 formatting\n",
    "\n",
    "plt.xticks([])  # Remove x-axis labels\n",
    "plt.tight_layout()  # Adjust layout to make room for x-axis labels\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea4c31-5b67-4d0b-8c04-b6f438f26da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: sorted_bigram_tuples = [(('lemma1', 'lemma2'), count1), (('lemma3', 'lemma4'), count2), ...]\n",
    "# Make sure `sorted_bigram_tuples` is sorted by the count in descending order.\n",
    "\n",
    "# Extract bigrams and their counts\n",
    "bigrams = [' '.join(bigram) for bigram, count in sorted_bigram_tuples]\n",
    "counts = [count for bigram, count in sorted_bigram_tuples]\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bigrams, counts, color='skyblue')\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title('Bigram Frequency Distribution')\n",
    "plt.xlabel('Bigrams')\n",
    "plt.ylabel('Counts')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to make room for x-axis labels\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e822de2-023a-4287-84c2-d9b76176cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text with nltk\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(test_text)\n",
    "\n",
    "# Analyze frequency\n",
    "fdist = FreqDist(tokens)\n",
    "print(fdist.most_common())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da01336-252b-4ce0-a1e2-7224e06eaf0f",
   "metadata": {},
   "source": [
    "Textstat:\n",
    "Readability Metrics: Calculates readability scores such as Flesch-Kincaid, Gunning Fog Index, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728462d-5015-406a-9dbb-edca1727e80d",
   "metadata": {},
   "source": [
    "Lexical Richness: Measures vocabulary diversity and richness (e.g., Type-Token Ratio, Hapax Legomena).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a7c1d-20c9-4e78-9c5c-da96400dfbe2",
   "metadata": {},
   "source": [
    "jupyter labextension install @jupyterlab/statusbar\n",
    "Once installed, you should see a status bar at the bottom of JupyterLab showing memory usage, CPU load, and kernel status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f16350-fbe4-4650-a426-9341004a7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK with Custom Regex Tokenizer\n",
    "\n",
    "# NLTK allows for more fine-grained control over tokenization using regular expressions. You can define custom patterns to handle specific cases.\n",
    "# Define a custom tokenizer with regex for contractions and punctuation\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"[\\w]+|[^\\w\\s]\", gaps=False)\n",
    "\n",
    "# Tokenize a sample sentence\n",
    "test_text = \"Don't stop—keep going! It's fascinating... isn't it?\"\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a5dce-aa23-4c6f-bb02-82ff463f16db",
   "metadata": {},
   "source": [
    "SpaCy Custom Tokenizer\n",
    "\n",
    "SpaCy allows you to modify its default tokenizer or add custom rules to handle specific cases like contractions or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667547be-a016-46aa-b791-cd8555acb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the tokenizer\n",
    "infixes = nlp_spacy.Defaults.infixes + [r'--', r'\\.\\.\\.']  # Add em dashes and ellipses as infixes\n",
    "infix_re = compile_infix_regex(infixes)\n",
    "nlp_spacy.tokenizer.infix_finditer = infix_re.finditer\n",
    "\n",
    "# Add custom rules for contractions\n",
    "special_cases = {\"don't\": [{\"ORTH\": \"do\"}, {\"ORTH\": \"n't\"}],\n",
    "                 \"it's\": [{\"ORTH\": \"it\"}, {\"ORTH\": \"'s\"}]}\n",
    "for case, rules in special_cases.items():\n",
    "    nlp_spacy.tokenizer.add_special_case(case, rules)\n",
    "\n",
    "# Tokenize a sample sentence\n",
    "test_text = \"Don't stop—keep going! It's fascinating... isn't it?\"\n",
    "doc = nlp_spacy(test_text)\n",
    "print([token.text for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fbbbb7-4740-4711-ac95-2e4424474f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sentences[:30]:\n",
    "    print (sentence.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b13baf-116c-4d1f-9aac-fe2e6317df99",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \" \".join([word.text for word in doc.sentences[5].words])\n",
    "print(string)\n",
    "plot_sentence_graph(doc.sentences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0e34e-8707-4082-b7df-dd70e1210e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
    "test_doc = nlp_spacy(test_text)\n",
    "\n",
    "# Tokenization and Lemmatization\n",
    "for token in test_doc:\n",
    "    print(f\"Token: {token.text}, Lemma: {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848292c-18f4-4be3-8cf5-2233f53c3efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d670db-06ae-47e0-8e37-94f520583a18",
   "metadata": {},
   "source": [
    "Post-Processing Tokens\n",
    "\n",
    "After tokenization, you might want to refine the tokens to ensure accuracy. This can involve:\n",
    "\n",
    "Merging Tokens: Combining tokens that were incorrectly split (e.g., splitting \"don't\" into \"do\" and \"n't\").\n",
    "\n",
    "Removing or Replacing Tokens: Removing unnecessary punctuation or normalizing specific characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0169c6a-0636-466c-90c0-b00cf664d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text with spacy\n",
    "test_text = \"Don't stop—keep going!\"\n",
    "test_doc = nlp_spacy(test_text)\n",
    "\n",
    "# Manually merge tokens if needed\n",
    "with test_doc.retokenize() as retokenizer:\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.text == \"n't\":\n",
    "            retokenizer.merge(test_doc[i-1:i+1])\n",
    "\n",
    "print([token.text for token in test_doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5244da-0525-49bb-b8e1-022a6d4d4752",
   "metadata": {},
   "source": [
    "Examine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d46c1-c629-4999-b8d0-49963b837430",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_number = 0\n",
    "sentence = doc.sentences[sentence_number]\n",
    "#sentence = tranche_docs[0][1].sentences[sentence_number]\n",
    "\n",
    "for word in sentence.words:\n",
    "    print(f\"word:  {word.text}  features:  {word.feats}\\n\")\n",
    "print(f\"sentence sentiment: {sentence.sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396134b-6c00-44f1-a365-9e88ddf0957e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98fd02-4523-4191-bb57-4f479ac79a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = book_text.splitlines()\n",
    "for i, line in enumerate(lines[:10]):\n",
    "    print(f'line {i}:{line}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502becf-0020-4617-be53-29fa2be83b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc.sentences[:2]:\n",
    "   for word_ref, word in enumerate(sentence.words):\n",
    "       print(f\"{word_ref+1}  Word: {word.text}, Lemma: {word.lemma}, POS: {word.pos}, Head: {word.head}, DepRel: {word.deprel}\")\n",
    "   print(\"\\n\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb4cab-74d2-4b65-ba77-1e8add4d41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentences = 3\n",
    "for i, sentence in enumerate(doc.sentences[:max_sentences]):\n",
    "    print(f\"Sentence {i + 1}: {sentence.text}\")\n",
    "    for word in sentence.words:\n",
    "        print(f\"  Word: {word.text}, POS: {word.pos}, Lemma: {word.lemma}\")\n",
    "    print()  # Print a blank line between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be16f4-ea45-4a39-9b6a-1a7f1cd8c1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707ce8e-3256-4dea-8a00-5e6568ce4251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156fa0c-9606-400c-ac74-4a9d3681ba6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc8338-dd9d-4aba-bb4d-c9cacecf9cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b46f56-01b0-43c3-bead-1c9aa4b5a614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b245ef0-4a14-4d43-984d-50e9e84c2b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add65f3-d03a-4d9f-95dd-85682519f2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef149b-52e4-4896-8b62-f4ed03853f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97302d4c-6867-4938-910b-a01f674e723f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a204ae8-88eb-4096-bc2b-a55fd66a4b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb1fce-f507-4f56-b0ad-2cebccaab215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518f73a-06b6-4ab6-b111-687142359a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d7f9b-3000-43a9-9f4f-db503782a961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152acc0-975c-4cb9-9680-429a5064887c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41583f60-c5a4-4be3-99ea-4c545dacbbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b20e03-e70b-4472-866b-09f32294b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9357d53-ba36-4e14-a3eb-3c1645c8c3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25faf51f-d880-42ad-b5e2-e9fcef0b0be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272825b5-f232-488b-b407-2fe14b7794a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d299f7-969c-40b4-bb4c-f2c2d1040fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493383b7-27c8-4ced-b12a-e83fd3182a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "542f7588-e0d9-460e-a77d-a1d6b09a78e6",
   "metadata": {},
   "source": [
    "Saving and loading stanza objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb5bf2-2d61-42a5-82bb-59787e6d82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"Never let me go stanza json file with invalid chars\"\n",
    "file_path = directory_path + save_name + \".json\"\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    # Write the opening of the JSON array\n",
    "    file.write('[\\n')\n",
    "    \n",
    "    # Process each sentence and write it as a separate JSON object\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        sentence_data = {\n",
    "            \"text\": sentence.text,\n",
    "            \"words\": [{\"text\": word.text, \"pos\": word.pos, \"lemma\": word.lemma} for word in sentence.words]\n",
    "            }\n",
    "        \n",
    "        # Write the JSON object to the file\n",
    "        json.dump(sentence_data, file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # If it's not the last sentence, add a comma\n",
    "        if i < len(doc.sentences) - 1:\n",
    "            file.write(',\\n')\n",
    "    \n",
    "    # Write the closing of the JSON array\n",
    "    file.write('\\n]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110b02a-aa66-452d-88a4-92965b7de683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "save_name = \"Never let me go stanza json file with invalid chars\"\n",
    "file_path = directory_path + save_name + \".json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    sentences_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d8f54-d9c1-40ac-a34c-2ab2d9c4c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually create a document-like structure from the loaded data\n",
    "# This is a simplification and not an exact replica of stanza.Document\n",
    "class SimpleDocument:\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "\n",
    "class SimpleSentence:\n",
    "    def __init__(self, text, words):\n",
    "        self.text = text\n",
    "        self.words = words\n",
    "\n",
    "class SimpleWord:\n",
    "    def __init__(self, text, pos, lemma):\n",
    "        self.text = text\n",
    "        self.pos = pos\n",
    "        self.lemma = lemma\n",
    "\n",
    "# Reconstruct the sentences\n",
    "reconstructed_sentences = []\n",
    "for sentence_data in sentences_data:\n",
    "    words = [SimpleWord(word['text'], word['pos'], word['lemma']) for word in sentence_data['words']]\n",
    "    sentence = SimpleSentence(sentence_data['text'], words)\n",
    "    reconstructed_sentences.append(sentence)\n",
    "\n",
    "# Create a simple document object\n",
    "reconstructed_doc = SimpleDocument(reconstructed_sentences)\n",
    "\n",
    "# Access the reconstructed data\n",
    "for sentence in reconstructed_doc.sentences:\n",
    "    print(f\"Sentence: {sentence.text}\")\n",
    "    for word in sentence.words:\n",
    "        print(f\"  Word: {word.text}, POS: {word.pos}, Lemma: {word.lemma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd35836-a535-41b5-94d0-5831b5b3eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16aa9da-64ea-4086-a76c-012f0029d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"Never let me go stanza csv file with invalid chars\"\n",
    "file_path = directory_path + save_name + \".csv\"\n",
    "# Save annotations to a CSV file\n",
    "with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Sentence\", \"Word\", \"POS\", \"Lemma\"])  # Header\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            writer.writerow([sentence.text, word.text, word.pos, word.lemma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc506af2-1ff4-4246-96eb-6bf4dde1561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bdf45e-9c4d-427c-957f-8723b937aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7e752-a535-4fa0-833c-026ca1e63036",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fbf722-98ca-4762-8074-4496e7068242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['Word', 'POS']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be97f1-fbcc-482b-876c-e6c3ab817408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Filtering rows where a certain condition is met\n",
    "filtered_df = df[df['POS'] == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006e510-1fb8-41b7-b5d1-6760e75dbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Grouping by a column and counting occurrences\n",
    "grouped_df = df.groupby('POS').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159e8ca-851c-4ac5-99aa-2d901bae32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sorting by a specific column\n",
    "sorted_df = df.sort_values(by='Lemma')\n",
    "print(sorted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71609cba-4232-4474-8ae5-b5b86fdd5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save modified data frame\n",
    "df.to_csv('modified_stanza_output.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600ba9a-37d9-427a-ad87-542d07c44449",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"Never let me go stanza text file with invalid chars\"\n",
    "file_path = directory_path + save_name + \".txt\"\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for sentence in doc.sentences:\n",
    "        file.write(f\"Sentence: {sentence.text}\\n\")\n",
    "        for word in sentence.words:\n",
    "            file.write(f\"  Word: {word.text}, POS: {word.pos}, Lemma: {word.lemma}\\n\")\n",
    "        file.write(\"\\n\")  # Blank line between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0e5a0-a074-486a-8a27-3df2385c71a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
